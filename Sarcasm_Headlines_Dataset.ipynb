{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of case study1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPMDZyjg6rLur4rP+zLXT/c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thanh-Hoo/CS114.K21/blob/master/Sarcasm_Headlines_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5PhDs2TS0rA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4bcc1a5b-1629-4170-e6ae-542fbbb1d513"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount( '/content/gdrive' )"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJt3m71cI2FM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHNW0ExsCXVY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "44b7be0a-9983-421c-a72b-133457bd368c"
      },
      "source": [
        "!unzip 'gdrive/My Drive/DATA_COLAB/BC_ML/news-headlines-dataset-for-sarcasm-detection.zip'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  gdrive/My Drive/DATA_COLAB/BC_ML/news-headlines-dataset-for-sarcasm-detection.zip\n",
            "replace Sarcasm_Headlines_Dataset.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZXM119JIP4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_1 = pd.read_json(\"Sarcasm_Headlines_Dataset.json\",lines=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umQnArT2i7OT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "5c827cd4-5064-4df7-ec38-06efaf8d85ba"
      },
      "source": [
        "df_2 = pd.read_csv(\"gdrive/My Drive/DATA_COLAB/BC_ML/test_data.csv\",)\n",
        "df_2"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BirthdayForBreonna Marks What Would've Been Br...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TheShowMustBePaused Was Eclipsed By #BlackOutT...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Again, Again, Again!' Exclaims Clapping, Grinn...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>And Then There Were 23,' Says Wayne Messam Cro...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>And Then Those 12 People Send It To 12 People ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2154</th>\n",
              "      <td>Zoom App Sued For Sharing User Data</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2155</th>\n",
              "      <td>Zoom Can Track Who's Not Paying Attention In Y...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2156</th>\n",
              "      <td>Zoom Crasher Becomes Too Engrossed In Sales Me...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2157</th>\n",
              "      <td>Zuckerberg Says He'll Review Policies That All...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2158</th>\n",
              "      <td>Zuckerberg Says Trump's Inflammatory Facebook ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2159 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               headline  is_sarcastic\n",
              "0     BirthdayForBreonna Marks What Would've Been Br...             0\n",
              "1     TheShowMustBePaused Was Eclipsed By #BlackOutT...             0\n",
              "2     Again, Again, Again!' Exclaims Clapping, Grinn...             1\n",
              "3     And Then There Were 23,' Says Wayne Messam Cro...             1\n",
              "4     And Then Those 12 People Send It To 12 People ...             1\n",
              "...                                                 ...           ...\n",
              "2154                Zoom App Sued For Sharing User Data             1\n",
              "2155  Zoom Can Track Who's Not Paying Attention In Y...             0\n",
              "2156  Zoom Crasher Becomes Too Engrossed In Sales Me...             1\n",
              "2157  Zuckerberg Says He'll Review Policies That All...             0\n",
              "2158  Zuckerberg Says Trump's Inflammatory Facebook ...             0\n",
              "\n",
              "[2159 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y82HBAGcIj1k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "c6ce2c46-7205-4639-c0d4-120a4f9c57dd"
      },
      "source": [
        "df_1.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_link</th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
              "      <td>former versace store clerk sues over secret 'b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
              "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
              "      <td>mom starting to fear son's web series closest ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
              "      <td>boehner just wants wife to listen, not come up...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
              "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                        article_link  ... is_sarcastic\n",
              "0  https://www.huffingtonpost.com/entry/versace-b...  ...            0\n",
              "1  https://www.huffingtonpost.com/entry/roseanne-...  ...            0\n",
              "2  https://local.theonion.com/mom-starting-to-fea...  ...            1\n",
              "3  https://politics.theonion.com/boehner-just-wan...  ...            1\n",
              "4  https://www.huffingtonpost.com/entry/jk-rowlin...  ...            0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmKZOcyaO7js",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "1ab5e3c3-948a-44d5-8012-9c046384b763"
      },
      "source": [
        "df_1.info()\n",
        "print(\"                  \")\n",
        "df_2.info()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 26709 entries, 0 to 26708\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   article_link  26709 non-null  object\n",
            " 1   headline      26709 non-null  object\n",
            " 2   is_sarcastic  26709 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 626.1+ KB\n",
            "                  \n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2159 entries, 0 to 2158\n",
            "Data columns (total 2 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   headline      2159 non-null   object\n",
            " 1   is_sarcastic  2159 non-null   int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 33.9+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQHfCRDnALnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_1=df_1.drop(labels=\"article_link\",axis=1)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cQsRubFAZDs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        },
        "outputId": "05fe3aa7-cc2d-4421-affa-ea547536f586"
      },
      "source": [
        "import numpy as np\n",
        "import string \n",
        "from string import digits\n",
        "from string import punctuation\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "for i in df_1['headline']:\n",
        "  i = str(i)\n",
        "\n",
        "for a in df_2['headline']:\n",
        "  a = str(a)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1BZrOeWSlxM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "4a81624b-26f7-40a2-b3f1-ae304a888664"
      },
      "source": [
        "df_1"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>headline</th>\n",
              "      <th>is_sarcastic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>former versace store clerk sues over secret 'b...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mom starting to fear son's web series closest ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>boehner just wants wife to listen, not come up...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26704</th>\n",
              "      <td>american politics in moral free-fall</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26705</th>\n",
              "      <td>america's best 20 hikes</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26706</th>\n",
              "      <td>reparations and obama</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26707</th>\n",
              "      <td>israeli ban targeting boycott supporters raise...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26708</th>\n",
              "      <td>gourmet gifts for the foodie 2014</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>26709 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                headline  is_sarcastic\n",
              "0      former versace store clerk sues over secret 'b...             0\n",
              "1      the 'roseanne' revival catches up to our thorn...             0\n",
              "2      mom starting to fear son's web series closest ...             1\n",
              "3      boehner just wants wife to listen, not come up...             1\n",
              "4      j.k. rowling wishes snape happy birthday in th...             0\n",
              "...                                                  ...           ...\n",
              "26704               american politics in moral free-fall             0\n",
              "26705                            america's best 20 hikes             0\n",
              "26706                              reparations and obama             0\n",
              "26707  israeli ban targeting boycott supporters raise...             0\n",
              "26708                  gourmet gifts for the foodie 2014             0\n",
              "\n",
              "[26709 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeuBZsb9ce6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#clean the data remove all digits and punctuation\n",
        "hl_clean_1 = []\n",
        "hl_clean_2 = []\n",
        "\n",
        "for i in df_1['headline']:\n",
        "    clean = i.translate(str.maketrans('', '', punctuation))\n",
        "    clean = clean.translate(str.maketrans('', '', digits))\n",
        "    hl_clean_1.append(clean)\n",
        "\n",
        "for a in df_2['headline']:\n",
        "    clean = a.translate(str.maketrans('', '', punctuation))\n",
        "    clean = clean.translate(str.maketrans('', '', digits))\n",
        "    hl_clean_2.append(clean)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loDNRkhvch-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tonization, break the headline to the lonely character like noun, pharse, ....\n",
        "hl_tokens_1 = []\n",
        "hl_tokens_2 = []\n",
        "for i in hl_clean_1:\n",
        "    i = i.split()\n",
        "    hl_tokens_1.append(i)\n",
        "\n",
        "for i in hl_clean_2:\n",
        "    i = i.split()\n",
        "    hl_tokens_2.append(i)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlpNfqJTcmCy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Init Lemmatizer 1\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "hl_lemmatized_1 = []\n",
        "for tokens in hl_tokens_1:\n",
        "    lemm = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens]\n",
        "    hl_lemmatized_1.append(lemm)\n",
        "\n",
        " # Init Lemmatizer 2 \n",
        "hl_lemmatized_2 = []\n",
        "for tokens in hl_tokens_2:\n",
        "    lemm = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens]\n",
        "    hl_lemmatized_2.append(lemm)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vxhYjJBcowu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb1fd6bc-5a60-4798-810e-aa42f522d7af"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "max_features = 2000\n",
        "max_token = len (hl_lemmatized_1)\n",
        "tokenizer = Tokenizer (num_words= max_features)\n",
        "tokenizer.fit_on_texts(hl_lemmatized_1)\n",
        "sequences = tokenizer.texts_to_sequences(hl_lemmatized_1)\n",
        "X_train = pad_sequences(sequences, maxlen = max_token)\n",
        "\n",
        "####\n",
        "\n",
        "max_features = 2000\n",
        "max_token = len (hl_lemmatized_1)\n",
        "tokenizer = Tokenizer (num_words= max_features)\n",
        "tokenizer.fit_on_texts(hl_lemmatized_2)\n",
        "sequences = tokenizer.texts_to_sequences(hl_lemmatized_2)\n",
        "X_test = pad_sequences(sequences, maxlen = max_token)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RYrtg3ccsFz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "360c9639-2e8b-4e97-d928-b51eb38cad3b"
      },
      "source": [
        "Y_train = df_1['is_sarcastic'].values\n",
        "Y_train = np.vstack(Y_train)\n",
        "#X_trai,X_tes,Y_trai,Y_tes = train_test_split(X_train,Y_train,test_size=0.3, random_state = 42)\n",
        "print (X_train)\n",
        "\n",
        "#######\n",
        "Y_test = df_2['is_sarcastic'].values\n",
        "Y_test = np.vstack(Y_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0 ... 1890    7 1985]\n",
            " [   0    0    0 ...  166    9  145]\n",
            " [   0    0    0 ...   41   14    1]\n",
            " ...\n",
            " [   0    0    0 ...    0    9   73]\n",
            " [   0    0    0 ...  686  715  425]\n",
            " [   0    0    0 ...  432    7    3]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvorCt-yQiUF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c00a6236-91ce-4c22-b165-b6d61d99934d"
      },
      "source": [
        "len(X_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2159"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwRKfV--YjK4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205
        },
        "outputId": "ed28540a-9628-4aeb-feec-92fd98cc41b7"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf  = RandomForestClassifier(n_estimators=300)\n",
        "clf.fit(X_train, Y_train)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=300,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E1qyA8Gc-cH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder,MinMaxScaler"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UesaAxDHerVt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "982ec769-652f-40e6-b421-5e66e2f63cda"
      },
      "source": [
        "le = LabelEncoder()\n",
        "y_result=le.fit_transform(Y_test)\n",
        "y_pred=clf.predict(X_test)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_label.py:251: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e_IjewCe9yi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9289882a-51b9-4765-8b93-139a967801fc"
      },
      "source": [
        "print(\"Result: \", (y_pred == y_result).tolist().count(True)/len(y_result))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Result:  0.6215840666975452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J95aYVHGc9Qz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "1f67184d-40d3-4011-c98d-5fad4a46ed9d"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_result,y_pred))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.76      0.68      1156\n",
            "           1       0.62      0.46      0.53      1003\n",
            "\n",
            "    accuracy                           0.62      2159\n",
            "   macro avg       0.62      0.61      0.61      2159\n",
            "weighted avg       0.62      0.62      0.61      2159\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}